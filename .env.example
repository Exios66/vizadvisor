# ─────────────────────────────────────────────
# VizAdvisor Environment Configuration
# Copy this file to .env and fill in your values.
# NEVER commit .env to source control.
# ─────────────────────────────────────────────

# LLM Provider: anthropic | openai
VITE_LLM_PROVIDER=anthropic

# Anthropic API Key (get from console.anthropic.com)
VITE_ANTHROPIC_API_KEY=your_anthropic_key_here

# OpenAI API Key (if using OpenAI provider)
VITE_OPENAI_API_KEY=your_openai_key_here

# Default model
VITE_DEFAULT_MODEL=claude-sonnet-4-6

# Optional: proxy backend URL for production
# When set, all LLM calls route through this URL instead of direct to API
VITE_API_PROXY_URL=

# Max tokens for LLM response (do not reduce below 1000)
VITE_MAX_TOKENS=2048

# Request timeout in milliseconds
VITE_REQUEST_TIMEOUT_MS=30000

# App environment
VITE_APP_ENV=development

# GitHub repository URL (for header link)
VITE_REPO_URL=https://github.com/your-username/vizadvisor
